from datetime import datetime
import os
from dotenv import load_dotenv
import openai
import streamlit as st
import pandas as pd
import re
from azure.storage.blob import BlobServiceClient

from contents_embedding import (
    read_blobs,
    remove_similar_questions_by_embedding,
    generate_rag_response,
    create_data_source,
    create_index,
    upload_documents,
    create_indexer,
    run_indexer,
)

# css ë¶ˆëŸ¬ì˜¤ê¸°
with open("static/style.css", "r", encoding="utf-8") as f:
    st.markdown(f"<style>{f.read()}</style>", unsafe_allow_html=True)

# í™˜ê²½ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°
load_dotenv()

# OpenAI library ì´ˆê¸°í™”
openai.api_key = os.getenv("OPENAI_API_KEY")
openai.azure_endpoint = os.getenv("AZURE_ENDPOINT")
openai.api_type = os.getenv("OPENAI_API_TYPE")
openai.api_version = os.getenv("OPENA_API_VERSION")
gpt_deployment_name = os.getenv("GPT_DEPLOYMENT_NAME")
embedding_deployment_name = os.getenv("EMBEDDING_DEPLOYMENT_NAME")

# search ê´€ë ¨ ì„¤ì •
gpt_deployment_name = os.getenv("GPT_DEPLOYMENT_NAME")
embedding_deployment_name = os.getenv("EMBEDDING_DEPLOYMENT_NAME")
search_key = os.getenv("SEARCH_KEY")
search_endpoint = os.getenv("SEARCH_ENDPOINT")
index_name = os.getenv("INDEX_NAME")
vector_dimensions = os.getenv("VECTOR_DIMENSIONS")
indexer_name = "indexer"
data_source_name = "taskgen-blob-datasource"

# Azure Storage ì„¤ì •
connection_string = os.getenv("AZURE_STORAGE_CONNECTION_STRING")
container_name = os.getenv("AZURE_CONTAINER_NAME")
blob_service_client = BlobServiceClient.from_connection_string(connection_string)
container_client = blob_service_client.get_container_client(container_name)


# gpt-4o-mini ëª¨ë¸ í˜¸ì¶œ
def get_openai_response(messages):
    """
    Azure OpenAI APIë¥¼ í˜¸ì¶œí•˜ì—¬ ì‘ë‹µì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
    """
    try:
        response = openai.chat.completions.create(
            model=gpt_deployment_name, messages=messages, temperature=0.9
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"Error: {str(e)}"


# í…ìŠ¤íŠ¸ ì²­í¬ í•¨ìˆ˜
def chunk_text(text, chunk_size=7500, overlap=200):
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end].strip()
        chunks.append(chunk)
        start += chunk_size - overlap
    return chunks


# Azure Blob ì—…ë¡œë“œ í•¨ìˆ˜
def upload_chunk_to_blob(blob_service_client, container, base_filename, chunks):
    container_client = blob_service_client.get_container_client(container)
    uploaded_files = []
    timestamp = datetime.utcnow().strftime("%Y%m%d%H%M%S")

    for i, chunk in enumerate(chunks, 1):
        blob_name = f"{base_filename}_chunk_{i}_{timestamp}.txt"
        container_client.upload_blob(name=blob_name, data=chunk, overwrite=True)
        uploaded_files.append(blob_name)

    return uploaded_files


# ì‘ë‹µ íŒŒì„œ í•¨ìˆ˜(ì—‘ì…€ ë‹¤ìš´ë¡œë“œìš©)
def parse_response(response):
    split_pattern = r"- (" + "|".join(fields) + r")\s*[:\-]?\s*"
    parts = re.split(split_pattern, response, flags=re.DOTALL)

    parsed = {field: "" for field in fields}
    i = 1
    while i < len(parts) - 1:
        field = parts[i].strip()
        value = parts[i + 1].strip()
        if field in parsed:
            parsed[field] = value
        i += 2
    return parsed


# ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

# --------------------------
# 1. ë¬¸ì œ ìœ í˜•ë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì •ì˜
# --------------------------
PROMPT_TEMPLATES = {
    "ì•Œê³ ë¦¬ì¦˜": {
        "system_message": """ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ì•Œê³ ë¦¬ì¦˜ ë¬¸ì œ ì¶œì œìì…ë‹ˆë‹¤.

        ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë¬¸ì œë¥¼ ìƒì„±í•˜ì„¸ìš”.
        ë‹¨, ì…ë ¥ê³¼ ì¶œë ¥ ì„¤ëª…ì— '-'(í•˜ì´í”ˆ) ë¬¸ìëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
        - ë¬¸ì œì œëª©:
        - ë¬¸ì œë‚´ìš©:
        - ì…ë ¥ì¡°ê±´:
        - ì…ë ¥ì˜ˆì‹œ:
        - ì¶œë ¥ì¡°ê±´:
        - ì¶œë ¥ì˜ˆì‹œ:
        - ë‹µì•ˆ:
        """,
        "fields": [
            "ë¬¸ì œì œëª©",
            "ë¬¸ì œë‚´ìš©",
            "ì…ë ¥ì¡°ê±´",
            "ì…ë ¥ì˜ˆì‹œ",
            "ì¶œë ¥ì¡°ê±´",
            "ì¶œë ¥ì˜ˆì‹œ",
            "ë‹µì•ˆ",
        ],
    },
    "ì •ë³´ì²˜ë¦¬ê¸°ì‚¬": {
        "system_message": """ë‹¹ì‹ ì€ ì •ë³´ì²˜ë¦¬ê¸°ì‚¬ ìê²©ì¦ ì‹œí—˜ ì¶œì œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

        ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ê°ê´€ì‹ ë¬¸ì œë¥¼ ìƒì„±í•˜ì„¸ìš”:
        - ë¬¸ì œì œëª©:
        - ë¬¸ì œë‚´ìš©:
        - ë³´ê¸°:
        - ë‹µì•ˆ:
        """,
        "fields": ["ë¬¸ì œì œëª©", "ë¬¸ì œë‚´ìš©", "ë³´ê¸°", "ë‹µì•ˆ"],
    },
}
st.title("Task Generator")

### UI: 2ë¶„í•  êµ¬ì„±
col1, col2 = st.columns([1, 2])  # ì™¼ìª½ 1, ì˜¤ë¥¸ìª½ 2 ë¹„ìœ¨

with col1:

    # íŒŒì¼ ì—…ë¡œë“œ ì˜ì—­
    st.subheader("ğŸ“„ ë¬¸ì„œ ì—…ë¡œë“œ")

    uploaded_file = st.file_uploader(
        "ì½”ì§€ê°€ í•™ìŠµí•  ë¬¸ì„œë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”!!", type=["pdf", "txt"]
    )

    # íŒŒì¼ ì—…ë¡œë“œ í›„ ì¸ë±ì‹± ë° ì¸ë±ì„œ ìƒì„±
    if uploaded_file is not None:

        # -----------------------------------
        # 2. parser & chunk
        # -----------------------------------
        file_contents = uploaded_file.read().decode("utf-8")
        file_name = os.path.splitext(uploaded_file.name)[0]

        chunk_size = st.slider(
            "ì²­í¬ í¬ê¸° (ë¬¸ì ìˆ˜)", min_value=200, max_value=7500, value=800, step=100
        )
        overlap = st.slider(
            "ì˜¤ë²„ë© (ì¤‘ë³µ ë¬¸ì ìˆ˜)", min_value=0, max_value=500, value=100, step=50
        )
        
        if st.button("ì—…ë¡œë“œ", use_container_width=True):

            try:
                chunks = chunk_text(
                    file_contents, chunk_size=chunk_size, overlap=overlap
                )
                blob_service_client = BlobServiceClient.from_connection_string(
                    connection_string
                )
                uploaded_files = upload_chunk_to_blob(
                    blob_service_client, container_name, file_name, chunks
                )
                st.success(
                    f"âœ… `{uploaded_file.name}` íŒŒì¼ì´ Blob Containerì— ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤."
                )
            except Exception as e:
                st.error(f"âŒ ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}")

        if st.button("ë¬¸ì„œ ë¶„ì„", use_container_width=True):
            with st.spinner("ë¬¸ì„œë¥¼ ë¶„ì„í•˜ê³  AI ê²€ìƒ‰ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤..."):
                try:
                    create_data_source()
                    create_index()
                    docs = read_blobs()

                    upload_documents(docs)

                    create_indexer(
                        search_endpoint=search_endpoint,
                        search_key=search_key,
                        index_name=index_name,
                        data_source_name="taskgen-blob-datasource",
                        indexer_name="taskgen-indexer",
                    )

                    run_indexer(
                        search_endpoint=search_endpoint,
                        search_key=search_key,
                        indexer_name="taskgen-indexer",
                    )

                    st.success(
                        "âœ… ë¬¸ì„œ ë¶„ì„ ì™„ë£Œ! ì´ì œ ì´ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì œë¥¼ ìƒì„±í•  ìˆ˜ ìˆì–´ìš”!"
                    )
                except Exception as e:
                    st.error(f"âŒ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

    ######### êµ¬ë¶„ì„  ì¶”ê°€ #########
    st.divider()
    ##############################

    # -----------------------------------
    # 2. Streamlit ì…ë ¥ ì˜ì—­
    # -----------------------------------
    st.subheader("ğŸš€ ë¬¸ì œ ìƒì„±")

    # ë¬¸ì œ ìœ í˜• ì„ íƒ ë“œë¡­ë‹¤ìš´ ì¶”ê°€
    problem_type = st.selectbox("ë¬¸ì œ ìœ í˜•ì„ ì„ íƒí•˜ì„¸ìš”:", ("ì •ë³´ì²˜ë¦¬ê¸°ì‚¬", "ì•Œê³ ë¦¬ì¦˜"))

    # ë¬¸ì œ ë‚œì´ë„ ì„ íƒ ë“œë¡­ë‹¤ìš´ ì¶”ê°€
    problem_level = st.selectbox(
        "ë¬¸ì œ ë‚œì´ë„ë¥¼ ì„ íƒí•˜ì„¸ìš”:", ("ëœë¤", "ì´ˆê¸‰", "ì¤‘ê¸‰", "ê³ ê¸‰")
    )

    # ë¬¸ì œ ê°œìˆ˜ ì…ë ¥ ë°›ê¸°
    problem_count = st.number_input(
        "ìƒì„±í•  ë¬¸ì œ ê°œìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš”:", min_value=0, max_value=100, value=2
    )

    detail = ""
    if problem_type == "ì •ë³´ì²˜ë¦¬ê¸°ì‚¬":
        # ì¶”ê°€ ë‚´ìš© ì…ë ¥
        detail = st.text_input("(ì„ íƒ) ìì‹ ì´ ë¶€ì¡±í•œ ì˜ì—­ì„ ì–˜ê¸°í•´ ë³´ì„¸ìš”:")

    # -----------------------------------
    # 3. ë¬¸ì œ ìƒì„± ë²„íŠ¼ í´ë¦­ ì‹œ
    # -----------------------------------

    # ë¬¸ì œ ìƒì„± ë²„íŠ¼
    if st.button("ë¬¸ì œ ìƒì„±"):

        # í”„ë¡¬í”„íŠ¸ ì„¤ì • ë¶ˆëŸ¬ì˜¤ê¸°
        selected_prompt = PROMPT_TEMPLATES.get(problem_type)
        system_message = selected_prompt["system_message"]
        fields = selected_prompt["fields"]

        # ìœ ì € ì…ë ¥ ë¬¸ì¥ êµ¬ì„±
        if detail:
            user_input = f"ì£¼ì œ: {detail}ê³¼ ê´€ë ¨ëœ ë¬¸ì œë¥¼ ìƒì„±í•´ ì£¼ì„¸ìš”."

        else:
            user_input = "ë¬¸ì œ ìƒì„±"

        st.session_state.messages.append({"role": "user", "content": user_input})

        assistant_responses = []

        with st.spinner("ì‘ë‹µì„ ê¸°ë‹¤ë¦¬ëŠ” ì¤‘..."):
            for i in range(problem_count):
                # ë¬¸ì œë§ˆë‹¤ ë‹¤ë¥¸ ë©”ì„¸ì§€ ìƒì„±
                dynamic_user_message = {
                    "role": "user",
                    "content": f"{i+1}ë²ˆì§¸ ë¬¸ì œì…ë‹ˆë‹¤. '{problem_type}' ìœ í˜•ì˜ '{problem_level}' ë‚œì´ë„ë¡œ '{detail}' ë¬¸ì œë¥¼ ìƒì„±í•´ ì£¼ì„¸ìš”. ë¬¸ì œ ì„¤ëª…ê³¼ ë‹µì•ˆì„ í¬í•¨í•´ ì£¼ì„¸ìš”.",
                }

                # system message ìƒˆë¡œ êµ¬ì„±
                message = [
                    {"role": "system", "content": system_message},
                    dynamic_user_message,
                ]

                # ìœ í˜•ì— ë”°ë¼ ì‘ë‹µ í•¨ìˆ˜ ë¶„ê¸°
                if problem_type == "ì•Œê³ ë¦¬ì¦˜":
                    response = get_openai_response(message)
                elif problem_type == "ì •ë³´ì²˜ë¦¬ê¸°ì‚¬":
                    response = generate_rag_response(message)
                else:
                    response = "âŒ ì§€ì›ë˜ì§€ ì•ŠëŠ” ë¬¸ì œ ìœ í˜•ì…ë‹ˆë‹¤."

                assistant_responses.append(response)
                st.chat_message("assistant").write(response)

            # ìœ í˜•ì— ë”°ë¼ ì‘ë‹µ í•¨ìˆ˜ ë¶„ê¸°
            parsed_data = [parse_response(r) for r in assistant_responses]
            df = pd.DataFrame(parsed_data)

            if problem_type == "ì•Œê³ ë¦¬ì¦˜":
                # ë°ì´í„° ì •ì œ
                df = df.applymap(
                    lambda x: (
                        x.replace("```python", "").replace("```", "")
                        if isinstance(x, str)
                        else x
                    )
                )

            df = remove_similar_questions_by_embedding(df, threshold=0.9)
            st.session_state["generated_df"] = df

# -----------------------------------
# 4. ë¬¸ì œ ìƒì„± ê²°ê³¼ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ
# -----------------------------------

with col2:
    st.subheader("ğŸ“‹ ìƒì„±ëœ ë¬¸ì œ")

    if "generated_df" in st.session_state:

        df_clean = st.session_state["generated_df"]

        csv = st.session_state["generated_df"].to_csv(index=False).encode("utf-8-sig")
        st.download_button(
            "ì—‘ì…€ ë‹¤ìš´ë¡œë“œ",
            data=csv,
            file_name="generated_problems.csv",
            mime="text/csv",
        )
        st.dataframe(df_clean)
    else:
        st.info("ì™¼ìª½ì—ì„œ ë¬¸ì œë¥¼ ìƒì„±í•˜ë©´ ê²°ê³¼ê°€ í‘œì‹œë©ë‹ˆë‹¤.")
        st.stop()  # ìƒì„±ëœ ë¬¸ì œ ì—†ìœ¼ë©´ ì•„ë˜ ì½”ë“œ ì‹¤í–‰ ì•ˆ ë˜ë„ë¡ ì°¨ë‹¨

    st.divider()

    # -----------------------------------
    # 5. ìƒì„±ëœ ë¬¸ì œ í’€ê¸°
    # -----------------------------------

    # ìƒíƒœ ì´ˆê¸°í™”
    if "current_question" not in st.session_state:
        st.session_state.current_question = 0

    if "user_answers" not in st.session_state or len(
        st.session_state.user_answers
    ) != len(df_clean):
        st.session_state.user_answers = [""] * len(df_clean)

    # í˜„ì¬ ë¬¸ì œ index
    idx = st.session_state.current_question
    row = df_clean.iloc[idx]

    st.subheader(f"ë¬¸ì œ {idx + 1} / {len(df_clean)}: {row['ë¬¸ì œì œëª©']}")
    st.write(row["ë¬¸ì œë‚´ìš©"])

    if "ë³´ê¸°" in row and pd.notna(row["ë³´ê¸°"]):
        cleaned_input = "\n".join(
            [line.strip() for line in str(row["ë³´ê¸°"]).splitlines()]
        )
        st.markdown(f"\n```\n{cleaned_input}\n```")
    # ì…ë ¥/ì¶œë ¥ ì˜ˆì‹œ (ê³µë°± ì œê±°)
    if "ì…ë ¥ì˜ˆì‹œ" in row and pd.notna(row["ì…ë ¥ì˜ˆì‹œ"]):
        cleaned_input = "\n".join(
            [line.strip() for line in str(row["ì…ë ¥ì˜ˆì‹œ"]).splitlines()]
        )
        st.markdown(f"**ì…ë ¥ ì˜ˆì‹œ:**\n```\n{cleaned_input}\n```")

    if "ì¶œë ¥ì˜ˆì‹œ" in row and pd.notna(row["ì¶œë ¥ì˜ˆì‹œ"]):
        cleaned_output = "\n".join(
            [line.strip() for line in str(row["ì¶œë ¥ì˜ˆì‹œ"]).splitlines()]
        )
        st.markdown(f"**ì¶œë ¥ ì˜ˆì‹œ:**\n```\n{cleaned_output}\n```")

    # ì •ë‹µ ì…ë ¥ í•„ë“œ
    st.session_state.user_answers[idx] = st.text_area(
        label="âœï¸ ì •ë‹µ ì…ë ¥:",
        key=f"user_answer_{idx}",
        value=st.session_state.user_answers[idx],
        height=100,
    )

    col1, col2, col3, _ = st.columns([1, 1, 1, 6])

    with col1:
        # ì±„ì  ë²„íŠ¼
        check = st.button("ì±„ì í•˜ê¸°", use_container_width=True)

    # í˜ì´ì§€ ì´ë™
    with col2:
        if st.button("â¬…ï¸ ì´ì „", disabled=(idx == 0), use_container_width=True):
            st.session_state.current_question = max(0, idx - 1)
    with col3:

        if st.button(
            "ë‹¤ìŒ â¡ï¸", disabled=(idx == len(df_clean) - 1), use_container_width=True
        ):
            st.session_state.current_question = min(len(df_clean) - 1, idx + 1)

    if check:
        correct = str(row.get("ë‹µì•ˆ", "")).strip()
        submitted = st.session_state.user_answers[idx].strip()

        if not correct:
            st.warning("ì •ë‹µ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì±„ì í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        elif submitted == correct:
            st.success("ì •ë‹µì…ë‹ˆë‹¤! âœ…")
        else:
            st.error("ì˜¤ë‹µì…ë‹ˆë‹¤ âŒ")
            st.text(f"ì •ë‹µ: {correct}")
