import os

import requests
import openai
import pandas as pd
import re
import json

from azure.storage.blob import BlobServiceClient
from azure.search.documents import SearchClient
from azure.search.documents.indexes import SearchIndexClient
from azure.search.documents.indexes import SearchIndexerClient
from azure.core.credentials import AzureKeyCredential
from azure.search.documents.indexes.models import (
    SearchIndex,
    SimpleField,
    SearchableField,
    VectorSearch,
    SearchField,
    HnswAlgorithmConfiguration,
    VectorSearchProfile,
    AzureOpenAIVectorizer,
    SearchFieldDataType,
    AzureOpenAIVectorizerParameters,
    SearchIndexer,
    SearchIndexerDataSourceConnection,
)
from dotenv import load_dotenv
from sklearn.metrics.pairwise import cosine_similarity

load_dotenv()  # í™˜ê²½ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°

# OpenAI library ì´ˆê¸°í™”
openai.api_key = os.getenv("OPENAI_API_KEY")
openai.azure_endpoint = os.getenv("AZURE_ENDPOINT")
openai.api_type = os.getenv("OPENAI_API_TYPE")
openai.api_version = os.getenv("OPENA_API_VERSION")

# search ê´€ë ¨ ì„¤ì •
gpt_deployment_name = os.getenv("GPT_DEPLOYMENT_NAME")
embedding_deployment_name = os.getenv("EMBEDDING_DEPLOYMENT_NAME")
search_key = os.getenv("SEARCH_KEY")
search_endpoint = os.getenv("SEARCH_ENDPOINT")
index_name = os.getenv("INDEX_NAME")
indexer_name = os.getenv("INDEXER_NAME")
data_source_name = os.getenv("DATA_SOURCE_NAME")

# ì¸ì¦ ê°ì²´ ìƒì„±
credential = AzureKeyCredential(search_key)

# í´ë¼ì´ì–¸íŠ¸ ìƒì„±
index_client = SearchIndexClient(endpoint=search_endpoint, credential=credential)
indexer_client = SearchIndexerClient(endpoint=search_endpoint, credential=credential)

# Azure Storage ì„¤ì •
connection_string = os.getenv("AZURE_STORAGE_CONNECTION_STRING")
container_name = os.getenv("AZURE_CONTAINER_NAME")
blob_service_client = BlobServiceClient.from_connection_string(connection_string)
container_client = blob_service_client.get_container_client(container_name)


# Blobì—ì„œ ë¬¸ì„œ ì½ê¸°
def read_blobs():
    documents = []
    for blob in container_client.list_blobs():
        blob_client = container_client.get_blob_client(blob)
        content = blob_client.download_blob().readall().decode("utf-8")
        documents.append({"id": blob.name, "content": content})
    return documents


# ìž„ë² ë”© ëª¨ë¸ í˜¸ì¶œ
def get_embeddings(texts: list[str]) -> list[list[float]]:
    response = openai.embeddings.create(
        input=texts,
        model=embedding_deployment_name,
    )
    return [d.embedding for d in response.data]


#  JSONL ì €ìž¥
def save_chunks_to_jsonl(docs, output_path):
    with open(output_path, "w", encoding="utf-8") as f:
        for doc in docs:
            f.write(json.dumps(doc, ensure_ascii=False) + "\n")


def create_data_source():
    indexer_client = SearchIndexerClient(
        endpoint=search_endpoint, credential=AzureKeyCredential(search_key)
    )

    data_source = SearchIndexerDataSourceConnection(
        name=data_source_name,
        type="azureblob",
        connection_string=connection_string,
        container={"name": container_name},
        description="Blob data source for taskgen documents",
    )

    indexer_client.create_or_update_data_source_connection(data_source)


# ì¸ë±ìŠ¤ ìƒì„±
def create_index():
    index_client = SearchIndexClient(
        endpoint=search_endpoint, credential=AzureKeyCredential(search_key)
    )

    vector_dimensions = 1536

    # searchablefiled > max 8192ìž
    fields = [
        SimpleField(name="id", type="Edm.String", key=True),
        SearchableField(name="content", type="Edm.String"),
        SearchField(
            name="contentvector",
            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),
            searchable=True,
            hidden=False,
            vector_search_dimensions=vector_dimensions,
            vector_search_profile_name="taskgenHnswProfile",
        ),
    ]
    # ë²¡í„° ê²€ìƒ‰ì„ ìœ„í•œ ì„¤ì •
    vector_search = VectorSearch(
        algorithms=[HnswAlgorithmConfiguration(name="taskgenHnsw")],
        profiles=[
            VectorSearchProfile(
                name="taskgenHnswProfile",
                algorithm_configuration_name="taskgenHnsw",
                vectorizer_name="taskgenVectorizer",
            )
        ],
        vectorizers=[
            AzureOpenAIVectorizer(
                vectorizer_name="taskgenVectorizer",
                parameters=AzureOpenAIVectorizerParameters(
                    resource_url=openai.azure_endpoint,
                    deployment_name=embedding_deployment_name,
                    model_name=embedding_deployment_name,
                    api_key=openai.api_key,
                ),
            )
        ],
    )

    # ì¸ë±ìŠ¤ ìƒì„±
    index = SearchIndex(name=index_name, fields=fields, vector_search=vector_search)

    # ì¸ë±ìŠ¤ê°€ ì¡´ìž¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒˆë¡œ ìƒì„±
    if index_name not in [i.name for i in index_client.list_indexes()]:
        index_client.create_index(index)
        print(f"Index '{index_name}' created successfully.")
    else:
        print(f"Index '{index_name}' already exists.")

    print("ì¸ë±ìŠ¤ ìƒì„±ê¹Œì§€ ì™„ë£Œ")


# blob íŒŒì¼ëª… ë‚´ íŠ¹ìˆ˜ë¬¸ìž ì œê±°
def sanitize_id(filename: str) -> str:
    # í™•ìž¥ìž ì œê±° í›„ ì•ˆì „í•œ ë¬¸ìžë§Œ ë‚¨ê¹€
    base = os.path.splitext(filename)[0]
    return re.sub(r"[^a-zA-Z0-9_\-=]", "_", base)


# ë¬¸ì„œ ì—…ë¡œë“œ
def upload_documents(documents):
    search_client = SearchClient(
        endpoint=search_endpoint,
        index_name=index_name,
        credential=AzureKeyCredential(search_key),
    )
    contents = [doc["content"] for doc in documents]
    vectors = get_embeddings(contents)

    enriched_docs = []
    for doc, vector in zip(documents, vectors):
        safe_id = sanitize_id(doc["id"])  # ì•ˆì „í•œ IDë¡œ ë³€í™˜
        enriched_docs.append(
            {"id": safe_id, "content": doc["content"], "contentvector": vector}
        )
    result = search_client.upload_documents(documents=enriched_docs)
    print(result)


# ì¸ë±ì„œ ìƒì„± í•¨ìˆ˜
def create_indexer(
    search_endpoint, search_key, index_name, data_source_name, indexer_name
):
    credential = AzureKeyCredential(search_key)
    indexer_client = SearchIndexerClient(
        endpoint=search_endpoint, credential=credential
    )

    indexer = SearchIndexer(
        name=indexer_name,
        data_source_name=data_source_name,
        target_index_name=index_name,
    )

    # ì¸ë±ì„œ ìƒì„±
    try:
        indexer_client.create_or_update_indexer(indexer)
        print(f"âœ… ì¸ë±ì„œ '{indexer_name}' ìƒì„± ë˜ëŠ” ì—…ë°ì´íŠ¸ ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ ì¸ë±ì„œ ìƒì„± ì‹¤íŒ¨: {e}")


# ì¸ë±ì„œ ì‹¤í–‰ í•¨ìˆ˜
def run_indexer(search_endpoint, search_key, indexer_name):
    credential = AzureKeyCredential(search_key)
    indexer_client = SearchIndexerClient(
        endpoint=search_endpoint, credential=credential
    )

    indexer_client.run_indexer(indexer_name)
    print(f"ðŸš€ ì¸ë±ì„œ '{indexer_name}' ìˆ˜ë™ ì‹¤í–‰ë¨")


# ë²¡í„° ê¸°ë°˜ RAG ê²€ìƒ‰ (Azure AI Search)
def search_similar_documents(query: str, k=3) -> list[str]:
    try:
        # 1. ì¿¼ë¦¬ ë²¡í„° ì–»ê¸°
        query_vector = get_embeddings([query])[0]

        # 2. REST API í˜¸ì¶œ ì„¤ì •
        url = f"{search_endpoint}/indexes/{index_name}/docs/search?api-version=2023-07-01-Preview"
        headers = {"Content-Type": "application/json", "api-key": search_key}
        data = {
            "vectors": [{"value": query_vector, "fields": "contentvector", "k": k}],
            "select": "content",
        }

        # 3. POST ìš”ì²­
        response = requests.post(url, headers=headers, json=data)

        if response.status_code != 200:
            print("âŒ REST ê²€ìƒ‰ ì‹¤íŒ¨:", response.status_code, response.text)
            return []

        # 4. ê²°ê³¼ íŒŒì‹±
        result_json = response.json()
        contents = [doc["content"] for doc in result_json.get("value", [])]
        print("ðŸ“„ ê²€ìƒ‰ ê²°ê³¼:", contents)
        return contents

    except Exception as e:
        print(f"âŒ ì˜ˆì™¸ ë°œìƒ: {e}")
        return []


def generate_rag_response(messages: list[dict]) -> str:
    try:
        # 1. userì˜ ì§ˆë¬¸ ì¶”ì¶œ
        user_query = next((m["content"] for m in messages if m["role"] == "user"), None)

        if not user_query:
            return "âŒ ì‚¬ìš©ìž ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤."

        # 2. ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
        print("ì§ˆë¬¸ë‚´ìš© í¬í•¨ gptì—ê²Œ ë˜ì§€ëŠ” ê°’", user_query)
        contexts = search_similar_documents(user_query)
        context_text = "\n\n".join(contexts)

        print("ðŸ“„ ë¬¸ì„œ context:", context_text[:200])

        # 3. contextë¥¼ system messageì— ì¶”ê°€
        enhanced_messages = [
            {
                "role": "system",
                "content": f"ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”:\n\n{context_text}",
            },
            *messages,
        ]

        # 4. GPT í˜¸ì¶œ
        response = openai.chat.completions.create(
            model=gpt_deployment_name,
            messages=enhanced_messages,
            temperature=0.7,
        )
        print("GPT ì‘ë‹µ:", response.choices[0].message.content.strip())
        return response.choices[0].message.content.strip()

    except Exception as e:
        print(f"âŒ GPT ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
        return "âŒ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."


# ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ ì œê±° í•¨ìˆ˜
def remove_similar_questions_by_embedding(
    df: pd.DataFrame, threshold: float = 0.9
) -> pd.DataFrame:
    texts = df["ë¬¸ì œë‚´ìš©"].tolist()
    embeddings = get_embeddings(texts)

    similarity_matrix = cosine_similarity(embeddings)
    seen = set()
    keep_indices = []

    for i in range(len(texts)):
        if i in seen:
            continue
        keep_indices.append(i)
        for j in range(i + 1, len(texts)):
            if similarity_matrix[i][j] > threshold:
                seen.add(j)

    return df.iloc[keep_indices].reset_index(drop=True)
